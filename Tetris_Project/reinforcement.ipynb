{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8b31c20-f236-4506-9e07-898dd6359423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import csv\n",
    "import os\n",
    "from copy import deepcopy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35acea92-f9de-4b6f-87cd-549d3380ce5f",
   "metadata": {},
   "source": [
    "# 1. Frame the problem\n",
    "Using the customer description, Define the problem your trying to solve in your own words (remember this is not technical but must be specific so the customer understands the project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d50df10f-91ec-40e9-98ff-0128732b22dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a neural network that plays tetris as long as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669adb4e-6b5d-4b1e-8487-7440aaff36dd",
   "metadata": {},
   "source": [
    "# 2. Get the Data \n",
    "Define how you recieved the data (provided, gathered..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4642b5a5-e013-4054-b333-768490f52361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(\"src/data_0.csv\")\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afa5fcb-fffb-4cf8-97f7-f4146aa75a5a",
   "metadata": {},
   "source": [
    "# 3. Explore the Data\n",
    "Gain insights into the data you have from step 2, making sure to identify any bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99bee722-9617-4ea4-9b78-e819ed3c10ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,6))\n",
    "# plt.plot(data[\"avg_fit\"], label=\"Average Fitness\", marker=\"o\")\n",
    "# plt.plot(data[\"top_fit\"], label=\"Top Fitness\", marker=\"s\")\n",
    "# plt.plot(data[\"elite_fit\"], label=\"Elite Fitness\", linestyle=\"--\")\n",
    "# plt.title(\"Tetris Reinforcement Learning Training Progress\")\n",
    "# plt.xlabel(\"Generation\")\n",
    "# plt.ylabel(\"Fitness Score\")\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# #Highest fitness is around the 23rd generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b85249f-bd12-4a86-b1f0-2e1a5f47a59f",
   "metadata": {},
   "source": [
    "# 4.Prepare the Data\n",
    "\n",
    "\n",
    "Apply any data transformations and explain what and why\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba4629da-29ec-4623-b248-205f673039bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using reinforcement learning so don't need parameters just the board\n",
    "def bool_to_np(board):\n",
    "    return np.array(board, dtype=np.float32).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046411aa-4cd5-4c97-959a-cf0282dd5f56",
   "metadata": {},
   "source": [
    "# 5. Model the data\n",
    "Using selected ML models, experment with your choices and describe your findings. Finish by selecting a Model to continue with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f50257b-ed08-44e6-b96e-a09056853d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RL_AI(nn.Module):\n",
    "    def __init__(self, input_size=240, hidden_sizes=[128, 256, 256, 128], output_size=1, lr=0.001):\n",
    "        super(RL_AI, self).__init__()\n",
    "        layers = []\n",
    "        last_size = input_size\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(last_size, h))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            last_size = h\n",
    "        layers.append(nn.Linear(last_size, output_size))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self._create_weights()\n",
    "\n",
    "    def _create_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, dtype=torch.float32)\n",
    "        if x.ndim == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        return self.model(x)\n",
    "\n",
    "    def valuate(self, board_np):\n",
    "        board_tensor = torch.tensor(board_np, dtype=torch.float32).unsqueeze(0)\n",
    "        return self.forward(board_tensor).item()\n",
    "\n",
    "    def get_best_move(self, board, piece):\n",
    "        best_x = -1\n",
    "        max_value = -float('inf')\n",
    "        best_piece = None\n",
    "\n",
    "        for i in range(4):\n",
    "            piece = piece.get_next_rotation()\n",
    "            for x in range(board.width):\n",
    "                try:\n",
    "                    y = board.drop_height(piece, x)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                board_copy = deepcopy(board.board)\n",
    "                for pos in piece.body:\n",
    "                    board_copy[y + pos[1]][x + pos[0]] = True\n",
    "\n",
    "                np_board = bool_to_np(board_copy)\n",
    "                value = self.valuate(np_board)\n",
    "\n",
    "                if value > max_value:\n",
    "                    max_value = value\n",
    "                    best_x = x\n",
    "                    best_piece = piece\n",
    "\n",
    "        return best_x, best_piece\n",
    "\n",
    "    def train_step(self, state, target_value):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        target_tensor = torch.tensor([[target_value]], dtype=torch.float32)\n",
    "        pred = self.forward(state_tensor)\n",
    "        loss = self.loss_fn(pred, target_tensor)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "#The model doesn't learn properly yet, however it simulates training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24554634-7a00-44a6-bf0a-6041fa6bf307",
   "metadata": {},
   "source": [
    "# 6. Fine Tune the Model\n",
    "\n",
    "With the select model descibe the steps taken to acheve the best results possible \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43109106-f3f7-4c29-8a6c-d9c63c591372",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RL_AI(nn.Module):\n",
    "    def __init__(self, input_size=240, hidden_sizes=[128, 256, 256, 128],\n",
    "                 output_size=1, lr=0.001, gamma=0.99, epsilon=0.2):\n",
    "        super(RL_AI, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon \n",
    "\n",
    "        layers = []\n",
    "        last = input_size\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(last, h))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            last = h\n",
    "        layers.append(nn.Linear(last, output_size))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, dtype=torch.float32)\n",
    "        if x.ndim == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        return self.model(x)\n",
    "\n",
    "    def valuate(self, board_np):\n",
    "        return self.forward(board_np).item()\n",
    "\n",
    "    def get_best_move(self, board, piece):\n",
    "        best_x = -1\n",
    "        best_piece = None\n",
    "        best_value = -float('inf')\n",
    "        moves = []\n",
    "\n",
    "        for r in range(4):\n",
    "            rotated_piece = piece if r == 0 else piece.get_next_rotation()\n",
    "            for x in range(board.width):\n",
    "                try:\n",
    "                    y = board.drop_height(rotated_piece, x)\n",
    "                except:\n",
    "                    continue\n",
    "                temp_board = deepcopy(board)\n",
    "                result = temp_board.place(x, y, rotated_piece)\n",
    "                if isinstance(result, Exception):\n",
    "                    continue\n",
    "                cleared = temp_board.clear_rows()\n",
    "                np_board = bool_to_np(temp_board.board)\n",
    "                reward = cleared * 100.0 - self.board_penalty(temp_board)\n",
    "                moves.append((x, rotated_piece, np_board, reward))\n",
    "\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(moves)[:2]\n",
    "\n",
    "        for x, p, np_board, reward in moves:\n",
    "            value = self.forward(np_board).item() + reward\n",
    "            if value > best_value:\n",
    "                best_value, best_x, best_piece = value, x, p\n",
    "\n",
    "        return best_x, best_piece\n",
    "\n",
    "    def board_penalty(self, board):\n",
    "        heights = board.heights\n",
    "        holes = self.count_holes(board.board)\n",
    "        bumpiness = sum(abs(heights[i] - heights[i+1]) for i in range(board.width-1))\n",
    "        return 0.4 * max(heights) + 0.7 * holes + 0.3 * bumpiness\n",
    "\n",
    "    def count_holes(self, board_array):\n",
    "        holes = 0\n",
    "        for col in range(len(board_array[0])):\n",
    "            block_found = False\n",
    "            for row in range(len(board_array)):\n",
    "                if board_array[row][col]:\n",
    "                    block_found = True\n",
    "                elif block_found:\n",
    "                    holes += 1\n",
    "        return holes\n",
    "\n",
    "    def train_step(self, state, reward, next_state=None, done=False):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        reward_tensor = torch.tensor([[reward]], dtype=torch.float32)\n",
    "        q_val = self.forward(state_tensor)\n",
    "\n",
    "        if next_state is not None:\n",
    "            next_state_tensor = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                q_next = self.forward(next_state_tensor)\n",
    "                q_target = reward_tensor + (0 if done else self.gamma * q_next)\n",
    "        else:\n",
    "            q_target = reward_tensor\n",
    "\n",
    "        loss = self.loss_fn(q_val, q_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "#Tried penalizing more and increasing the epsilon rate but it is still not learning properly due to lack of replay function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01721554-2e2f-48c5-ac6b-a71269d3d565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def bool_to_np_trimmed(board):\n",
    "    trimmed_board = board[:20] \n",
    "    return np.array(trimmed_board, dtype=np.float32).flatten()\n",
    "\n",
    "class DQN_AI(nn.Module):\n",
    "    def __init__(self, state_dim=200, action_dim=10, hidden_sizes=[128, 128], lr=0.001, gamma=0.99, epsilon=0.1):\n",
    "        super(DQN_AI, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        layers = []\n",
    "        last = state_dim\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(last, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            last = h\n",
    "        layers.append(nn.Linear(last, action_dim))\n",
    "        self.model = nn.Sequential(*layers).to(device)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.memory = []\n",
    "        self.batch_size = 64\n",
    "        self.max_memory = 10000\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, dtype=torch.float32, device=device)\n",
    "        if x.ndim == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        return self.model(x)\n",
    "\n",
    "    def act(self, state):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.forward(state_tensor)\n",
    "            return int(torch.argmax(q_values, dim=1).item())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        if len(self.memory) >= self.max_memory:\n",
    "            self.memory.pop(0)\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.tensor(states, dtype=torch.float32, device=device)\n",
    "        actions = torch.tensor(actions, dtype=torch.long, device=device).unsqueeze(1)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32, device=device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "        q_vals = self.model(states).gather(1, actions)\n",
    "        with torch.no_grad():\n",
    "            q_next = self.model(next_states).max(1)[0].unsqueeze(1)\n",
    "            q_target = rewards + (1 - dones) * self.gamma * q_next\n",
    "\n",
    "        loss = self.loss_fn(q_vals, q_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "#Found out that reinforcement learning needs to be combined with QN learning to learn properly, currently still doesn't learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f034a666-9319-48b8-a860-5d91222a51d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def bool_to_np_trimmed(board):\n",
    "\n",
    "    trimmed_board = board[:20] \n",
    "    return np.array(trimmed_board, dtype=np.float32).flatten()\n",
    "\n",
    "def compute_board_features(board_obj):\n",
    "    flat_board = bool_to_np_trimmed(board_obj.board)\n",
    "    heights = board_obj.heights\n",
    "    holes = 0\n",
    "    for col in range(board_obj.width):\n",
    "        block_found = False\n",
    "        for row in range(board_obj.height):\n",
    "            if board_obj.board[row][col]:\n",
    "                block_found = True\n",
    "            elif block_found:\n",
    "                holes += 1\n",
    "    bumpiness = sum(abs(heights[i] - heights[i + 1]) for i in range(len(heights) - 1))\n",
    "    return np.concatenate([flat_board, np.array(heights, dtype=np.float32), np.array([holes, bumpiness], dtype=np.float32)])\n",
    "\n",
    "\n",
    "class DQN_AI(nn.Module):\n",
    "    def __init__(self, state_dim=212, action_dim=10, hidden_sizes=[128, 128], lr=0.001, gamma=0.99, epsilon_start=1.0, epsilon_end=0.1):\n",
    "        super(DQN_AI, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        # Q-network\n",
    "        layers = []\n",
    "        last = state_dim\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(last, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            last = h\n",
    "        layers.append(nn.Linear(last, action_dim))\n",
    "        self.model = nn.Sequential(*layers).to(device)\n",
    "\n",
    "        # Target network\n",
    "        self.target_model = deepcopy(self.model)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.memory = []\n",
    "        self.batch_size = 128\n",
    "        self.max_memory = 10000\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, dtype=torch.float32, device=device)\n",
    "        if x.ndim == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        return self.model(x)\n",
    "\n",
    "    def act(self, state, training=True):\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.forward(state)\n",
    "            return int(torch.argmax(q_values, dim=1).item())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        if len(self.memory) >= self.max_memory:\n",
    "            self.memory.pop(0)\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.tensor(states, dtype=torch.float32, device=device)\n",
    "        actions = torch.tensor(actions, dtype=torch.long, device=device).unsqueeze(1)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32, device=device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "        q_vals = self.model(states).gather(1, actions)\n",
    "        with torch.no_grad():\n",
    "            q_next = self.target_model(next_states).max(1)[0].unsqueeze(1)\n",
    "            q_target = rewards + (1 - dones) * self.gamma * q_next\n",
    "\n",
    "        loss = self.loss_fn(q_vals, q_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def decay_epsilon(self, episode, max_episodes):\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon_start * (1 - episode / max_episodes))\n",
    "\n",
    "#Implemented eplison decay and the model properly learns now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f8941f-ca82-4679-b1b4-f6dbf42e74d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def bool_to_np_trimmed(board):\n",
    "    trimmed_board = board[:20]  # keep top 20 rows\n",
    "    return np.array(trimmed_board, dtype=np.float32).flatten()\n",
    "\n",
    "def compute_board_features(board_obj):\n",
    "    flat_board = bool_to_np_trimmed(board_obj.board)\n",
    "    heights = board_obj.heights\n",
    "    holes = 0\n",
    "    for col in range(board_obj.width):\n",
    "        block_found = False\n",
    "        for row in range(board_obj.height):\n",
    "            if board_obj.board[row][col]:\n",
    "                block_found = True\n",
    "            elif block_found:\n",
    "                holes += 1\n",
    "    bumpiness = sum(abs(heights[i] - heights[i + 1]) for i in range(len(heights) - 1))\n",
    "    return np.concatenate([flat_board, np.array(heights, dtype=np.float32), np.array([holes, bumpiness], dtype=np.float32)])\n",
    "\n",
    "class DQN_AI(nn.Module):\n",
    "    def __init__(self, state_dim=212, action_dim=10, hidden_sizes=[128, 128], lr=0.001, gamma=0.99, epsilon_start=1.0, epsilon_end=0.1):\n",
    "        super(DQN_AI, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        # Q-network\n",
    "        layers = []\n",
    "        last = state_dim\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(last, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            last = h\n",
    "        layers.append(nn.Linear(last, action_dim))\n",
    "        self.model = nn.Sequential(*layers).to(device)\n",
    "\n",
    "        # Target network\n",
    "        self.target_model = deepcopy(self.model)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.memory = []\n",
    "        self.batch_size = 128\n",
    "        self.max_memory = 10000\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, dtype=torch.float32, device=device)\n",
    "        if x.ndim == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        return self.model(x)\n",
    "\n",
    "    def act(self, state, training=True):\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.forward(state)\n",
    "            return int(torch.argmax(q_values, dim=1).item())\n",
    "\n",
    "    def get_best_move(self, board_obj, piece):\n",
    "        best_value = -float('inf')\n",
    "        best_x = 0\n",
    "        for x in range(board_obj.width):\n",
    "            try:\n",
    "                y = board_obj.drop_height(piece, x)\n",
    "                temp_board = board_obj.clone()  \n",
    "                temp_board.place(x, y, piece)\n",
    "                features = compute_board_features(temp_board)\n",
    "                value = self.forward(features).item()\n",
    "                if value > best_value:\n",
    "                    best_value = value\n",
    "                    best_x = x\n",
    "            except:\n",
    "                continue \n",
    "        return best_x, piece\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        if len(self.memory) >= self.max_memory:\n",
    "            self.memory.pop(0)\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.tensor(states, dtype=torch.float32, device=device)\n",
    "        actions = torch.tensor(actions, dtype=torch.long, device=device).unsqueeze(1)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32, device=device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "        q_vals = self.model(states).gather(1, actions)\n",
    "        with torch.no_grad():\n",
    "            q_next = self.target_model(next_states).max(1)[0].unsqueeze(1)\n",
    "            q_target = rewards + (1 - dones) * self.gamma * q_next\n",
    "\n",
    "        loss = self.loss_fn(q_vals, q_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def decay_epsilon(self, episode, max_episodes):\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon_start * (1 - episode / max_episodes))\n",
    "\n",
    "# Implemented best move function so the model plays the game properly after training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac79c491-c2bd-401d-a862-1eb3b7aa2f9d",
   "metadata": {},
   "source": [
    "# 7. Present\n",
    "In a customer facing Document provide summary of finding and detail approach taken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "54a774ea-14f9-47a4-9582-443004ab4c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tasked with making a neural network that plays tetris for as long as possible  \\nThe data was outdated and using reinforcement learning, so do not need to check parameters\\nWent through 4 iterations:\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Tasked with making a neural network that plays tetris for as long as possible  \n",
    "The data was outdated and using reinforcement learning, so do not need to check parameters\n",
    "Went through 6 iterations:\n",
    "1: It could simulate gameplay through a simple neural network but could not learn\n",
    "2: It could evaluate the moves better since the parameters such as epsilon were increased, but could not learn\n",
    "3: Added q learning so it could remember and learn through the replay function\n",
    "4: Implemented missing epislon decay function to update the rewards\n",
    "5: Reimplemented the best move function so the model can be tested\n",
    "6: Work in progress, currently attempting to train the model with 50000 episodes, currently trained at 5000 and runs for a second\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909336b7-3c7e-4eda-b3ad-aee02adfc736",
   "metadata": {},
   "source": [
    "# 8. Launch the Model System\n",
    "Define your production run code, This should be self susficent and require only your model pramaters \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2de2ebbc-4fe1-4141-8f28-37817e4f4b84",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Game' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTest -> Pieces: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpieces_dropped\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Rows: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrows_cleared\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreward\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Inference code which loads the model, called test.py in actual project (tetris_a/src) and incoroprates the game, board and piece files from the tetris game\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# https://lab2.check-ai.com/user/260721/lab/tree/Machine-Learning-AI-Class/tetris_a/src/test.py\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36minference\u001b[39m\u001b[34m(agent)\u001b[39m\n\u001b[32m      6\u001b[39m     agent = DQN_AI(\u001b[32m212\u001b[39m, \u001b[32m10\u001b[39m)\n\u001b[32m      7\u001b[39m     agent.load_state_dict(torch.load(\u001b[33m'\u001b[39m\u001b[33magent.pth\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m game = \u001b[43mGame\u001b[49m(mode=\u001b[33m\"\u001b[39m\u001b[33mDQN\u001b[39m\u001b[33m\"\u001b[39m, agent=agent)\n\u001b[32m      9\u001b[39m pieces_dropped, rows_cleared = game.run()\n\u001b[32m     10\u001b[39m reward = pieces_dropped + rows_cleared\n",
      "\u001b[31mNameError\u001b[39m: name 'Game' is not defined"
     ]
    }
   ],
   "source": [
    "#from game import Game\n",
    "import torch\n",
    "\n",
    "def inference(agent=None):\n",
    "    if agent is None:\n",
    "        agent = DQN_AI(212, 10)\n",
    "        agent.load_state_dict(torch.load('5000EpisodeAgent.pth'))\n",
    "    game = Game(mode=\"DQN\", agent=agent)\n",
    "    pieces_dropped, rows_cleared = game.run()\n",
    "    reward = pieces_dropped + rows_cleared\n",
    "    print(f\"Test -> Pieces: {pieces_dropped}, Rows: {rows_cleared}, Reward: {reward}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    inference()\n",
    "\n",
    "\n",
    "# Inference code which loads the model, called test.py in actual project (tetris_a/src) and incoroprates the game, board and piece files from the tetris game\n",
    "# https://lab2.check-ai.com/user/260721/lab/tree/Machine-Learning-AI-Class/tetris_a/src/test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747c058d-8129-4fe8-be97-10cd0c670966",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
