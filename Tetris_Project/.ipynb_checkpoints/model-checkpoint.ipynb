{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8b31c20-f236-4506-9e07-898dd6359423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import csv\n",
    "import os\n",
    "from copy import deepcopy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35acea92-f9de-4b6f-87cd-549d3380ce5f",
   "metadata": {},
   "source": [
    "# 1. Frame the problem\n",
    "Using the customer description, Define the problem your trying to solve in your own words (remember this is not technical but must be specific so the customer understands the project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d50df10f-91ec-40e9-98ff-0128732b22dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a neural network that plays tetris as long as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669adb4e-6b5d-4b1e-8487-7440aaff36dd",
   "metadata": {},
   "source": [
    "# 2. Get the Data \n",
    "Define how you recieved the data (provided, gathered..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4642b5a5-e013-4054-b333-768490f52361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(\"src/data_0.csv\")\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afa5fcb-fffb-4cf8-97f7-f4146aa75a5a",
   "metadata": {},
   "source": [
    "# 3. Explore the Data\n",
    "Gain insights into the data you have from step 2, making sure to identify any bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99bee722-9617-4ea4-9b78-e819ed3c10ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,6))\n",
    "# plt.plot(data[\"avg_fit\"], label=\"Average Fitness\", marker=\"o\")\n",
    "# plt.plot(data[\"top_fit\"], label=\"Top Fitness\", marker=\"s\")\n",
    "# plt.plot(data[\"elite_fit\"], label=\"Elite Fitness\", linestyle=\"--\")\n",
    "# plt.title(\"Tetris Reinforcement Learning Training Progress\")\n",
    "# plt.xlabel(\"Generation\")\n",
    "# plt.ylabel(\"Fitness Score\")\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# #Highest fitness is around the 23rd generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b85249f-bd12-4a86-b1f0-2e1a5f47a59f",
   "metadata": {},
   "source": [
    "# 4.Prepare the Data\n",
    "\n",
    "\n",
    "Apply any data transformations and explain what and why\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba4629da-29ec-4623-b248-205f673039bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using reinforcement learning so don't need parameters just the board\n",
    "def bool_to_np(board):\n",
    "    return np.array(board, dtype=np.float32).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046411aa-4cd5-4c97-959a-cf0282dd5f56",
   "metadata": {},
   "source": [
    "# 5. Model the data\n",
    "Using selected ML models, experment with your choices and describe your findings. Finish by selecting a Model to continue with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f50257b-ed08-44e6-b96e-a09056853d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RL_AI(nn.Module):\n",
    "    def __init__(self, input_size=240, hidden_sizes=[128, 256, 256, 128], output_size=1, lr=0.001):\n",
    "        super(RL_AI, self).__init__()\n",
    "        layers = []\n",
    "        last_size = input_size\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(last_size, h))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            last_size = h\n",
    "        layers.append(nn.Linear(last_size, output_size))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self._create_weights()\n",
    "\n",
    "    def _create_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, dtype=torch.float32)\n",
    "        if x.ndim == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        return self.model(x)\n",
    "\n",
    "    def valuate(self, board_np):\n",
    "        board_tensor = torch.tensor(board_np, dtype=torch.float32).unsqueeze(0)\n",
    "        return self.forward(board_tensor).item()\n",
    "\n",
    "    def get_best_move(self, board, piece):\n",
    "        best_x = -1\n",
    "        max_value = -float('inf')\n",
    "        best_piece = None\n",
    "\n",
    "        for i in range(4):\n",
    "            piece = piece.get_next_rotation()\n",
    "            for x in range(board.width):\n",
    "                try:\n",
    "                    y = board.drop_height(piece, x)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                board_copy = deepcopy(board.board)\n",
    "                for pos in piece.body:\n",
    "                    board_copy[y + pos[1]][x + pos[0]] = True\n",
    "\n",
    "                np_board = bool_to_np(board_copy)\n",
    "                value = self.valuate(np_board)\n",
    "\n",
    "                if value > max_value:\n",
    "                    max_value = value\n",
    "                    best_x = x\n",
    "                    best_piece = piece\n",
    "\n",
    "        return best_x, best_piece\n",
    "\n",
    "    def train_step(self, state, target_value):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        target_tensor = torch.tensor([[target_value]], dtype=torch.float32)\n",
    "        pred = self.forward(state_tensor)\n",
    "        loss = self.loss_fn(pred, target_tensor)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "#The model doesn't learn properly yet, however it simulates training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24554634-7a00-44a6-bf0a-6041fa6bf307",
   "metadata": {},
   "source": [
    "# 6. Fine Tune the Model\n",
    "\n",
    "With the select model descibe the steps taken to acheve the best results possible \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43109106-f3f7-4c29-8a6c-d9c63c591372",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RL_AI(nn.Module):\n",
    "    def __init__(self, input_size=240, hidden_sizes=[128, 256, 256, 128],\n",
    "                 output_size=1, lr=0.001, gamma=0.99, epsilon=0.2):\n",
    "        super(RL_AI, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon \n",
    "\n",
    "        layers = []\n",
    "        last = input_size\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(last, h))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            last = h\n",
    "        layers.append(nn.Linear(last, output_size))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, dtype=torch.float32)\n",
    "        if x.ndim == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        return self.model(x)\n",
    "\n",
    "    def valuate(self, board_np):\n",
    "        return self.forward(board_np).item()\n",
    "\n",
    "    def get_best_move(self, board, piece):\n",
    "        best_x = -1\n",
    "        best_piece = None\n",
    "        best_value = -float('inf')\n",
    "        moves = []\n",
    "\n",
    "        for r in range(4):\n",
    "            rotated_piece = piece if r == 0 else piece.get_next_rotation()\n",
    "            for x in range(board.width):\n",
    "                try:\n",
    "                    y = board.drop_height(rotated_piece, x)\n",
    "                except:\n",
    "                    continue\n",
    "                temp_board = deepcopy(board)\n",
    "                result = temp_board.place(x, y, rotated_piece)\n",
    "                if isinstance(result, Exception):\n",
    "                    continue\n",
    "                cleared = temp_board.clear_rows()\n",
    "                np_board = bool_to_np(temp_board.board)\n",
    "                reward = cleared * 100.0 - self.board_penalty(temp_board)\n",
    "                moves.append((x, rotated_piece, np_board, reward))\n",
    "\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(moves)[:2]\n",
    "\n",
    "        for x, p, np_board, reward in moves:\n",
    "            value = self.forward(np_board).item() + reward\n",
    "            if value > best_value:\n",
    "                best_value, best_x, best_piece = value, x, p\n",
    "\n",
    "        return best_x, best_piece\n",
    "\n",
    "    def board_penalty(self, board):\n",
    "        heights = board.heights\n",
    "        holes = self.count_holes(board.board)\n",
    "        bumpiness = sum(abs(heights[i] - heights[i+1]) for i in range(board.width-1))\n",
    "        return 0.4 * max(heights) + 0.7 * holes + 0.3 * bumpiness\n",
    "\n",
    "    def count_holes(self, board_array):\n",
    "        holes = 0\n",
    "        for col in range(len(board_array[0])):\n",
    "            block_found = False\n",
    "            for row in range(len(board_array)):\n",
    "                if board_array[row][col]:\n",
    "                    block_found = True\n",
    "                elif block_found:\n",
    "                    holes += 1\n",
    "        return holes\n",
    "\n",
    "    def train_step(self, state, reward, next_state=None, done=False):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        reward_tensor = torch.tensor([[reward]], dtype=torch.float32)\n",
    "        q_val = self.forward(state_tensor)\n",
    "\n",
    "        if next_state is not None:\n",
    "            next_state_tensor = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                q_next = self.forward(next_state_tensor)\n",
    "                q_target = reward_tensor + (0 if done else self.gamma * q_next)\n",
    "        else:\n",
    "            q_target = reward_tensor\n",
    "\n",
    "        loss = self.loss_fn(q_val, q_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "#Tried penalizing more and increasing the epsilon rate but it is still not learning properly due to lack of replay function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01721554-2e2f-48c5-ac6b-a71269d3d565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def bool_to_np_trimmed(board):\n",
    "    trimmed_board = board[:20] \n",
    "    return np.array(trimmed_board, dtype=np.float32).flatten()\n",
    "\n",
    "class DQN_AI(nn.Module):\n",
    "    def __init__(self, state_dim=200, action_dim=10, hidden_sizes=[128, 128], lr=0.001, gamma=0.99, epsilon=0.1):\n",
    "        super(DQN_AI, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        layers = []\n",
    "        last = state_dim\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(last, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            last = h\n",
    "        layers.append(nn.Linear(last, action_dim))\n",
    "        self.model = nn.Sequential(*layers).to(device)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.memory = []\n",
    "        self.batch_size = 64\n",
    "        self.max_memory = 10000\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, dtype=torch.float32, device=device)\n",
    "        if x.ndim == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        return self.model(x)\n",
    "\n",
    "    def act(self, state):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.forward(state_tensor)\n",
    "            return int(torch.argmax(q_values, dim=1).item())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        if len(self.memory) >= self.max_memory:\n",
    "            self.memory.pop(0)\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.tensor(states, dtype=torch.float32, device=device)\n",
    "        actions = torch.tensor(actions, dtype=torch.long, device=device).unsqueeze(1)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32, device=device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "        q_vals = self.model(states).gather(1, actions)\n",
    "        with torch.no_grad():\n",
    "            q_next = self.model(next_states).max(1)[0].unsqueeze(1)\n",
    "            q_target = rewards + (1 - dones) * self.gamma * q_next\n",
    "\n",
    "        loss = self.loss_fn(q_vals, q_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "#Found out that reinforcement learning needs to be combined with QN learning to learn properly, currently still doesn't learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f034a666-9319-48b8-a860-5d91222a51d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def bool_to_np_trimmed(board):\n",
    "\n",
    "    trimmed_board = board[:20] \n",
    "    return np.array(trimmed_board, dtype=np.float32).flatten()\n",
    "\n",
    "def compute_board_features(board_obj):\n",
    "    flat_board = bool_to_np_trimmed(board_obj.board)\n",
    "    heights = board_obj.heights\n",
    "    holes = 0\n",
    "    for col in range(board_obj.width):\n",
    "        block_found = False\n",
    "        for row in range(board_obj.height):\n",
    "            if board_obj.board[row][col]:\n",
    "                block_found = True\n",
    "            elif block_found:\n",
    "                holes += 1\n",
    "    bumpiness = sum(abs(heights[i] - heights[i + 1]) for i in range(len(heights) - 1))\n",
    "    return np.concatenate([flat_board, np.array(heights, dtype=np.float32), np.array([holes, bumpiness], dtype=np.float32)])\n",
    "\n",
    "\n",
    "class DQN_AI(nn.Module):\n",
    "    def __init__(self, state_dim=212, action_dim=10, hidden_sizes=[128, 128], lr=0.001, gamma=0.99, epsilon_start=1.0, epsilon_end=0.1):\n",
    "        super(DQN_AI, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        # Q-network\n",
    "        layers = []\n",
    "        last = state_dim\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(last, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            last = h\n",
    "        layers.append(nn.Linear(last, action_dim))\n",
    "        self.model = nn.Sequential(*layers).to(device)\n",
    "\n",
    "        # Target network\n",
    "        self.target_model = deepcopy(self.model)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.memory = []\n",
    "        self.batch_size = 128\n",
    "        self.max_memory = 10000\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, dtype=torch.float32, device=device)\n",
    "        if x.ndim == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        return self.model(x)\n",
    "\n",
    "    def act(self, state, training=True):\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.forward(state)\n",
    "            return int(torch.argmax(q_values, dim=1).item())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        if len(self.memory) >= self.max_memory:\n",
    "            self.memory.pop(0)\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.tensor(states, dtype=torch.float32, device=device)\n",
    "        actions = torch.tensor(actions, dtype=torch.long, device=device).unsqueeze(1)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32, device=device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "        q_vals = self.model(states).gather(1, actions)\n",
    "        with torch.no_grad():\n",
    "            q_next = self.target_model(next_states).max(1)[0].unsqueeze(1)\n",
    "            q_target = rewards + (1 - dones) * self.gamma * q_next\n",
    "\n",
    "        loss = self.loss_fn(q_vals, q_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def decay_epsilon(self, episode, max_episodes):\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon_start * (1 - episode / max_episodes))\n",
    "\n",
    "#Implemented eplison decay and the model properly learns now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f8941f-ca82-4679-b1b4-f6dbf42e74d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def bool_to_np_trimmed(board):\n",
    "    trimmed_board = board[:20]  # keep top 20 rows\n",
    "    return np.array(trimmed_board, dtype=np.float32).flatten()\n",
    "\n",
    "def compute_board_features(board_obj):\n",
    "    flat_board = bool_to_np_trimmed(board_obj.board)\n",
    "    heights = board_obj.heights\n",
    "    holes = 0\n",
    "    for col in range(board_obj.width):\n",
    "        block_found = False\n",
    "        for row in range(board_obj.height):\n",
    "            if board_obj.board[row][col]:\n",
    "                block_found = True\n",
    "            elif block_found:\n",
    "                holes += 1\n",
    "    bumpiness = sum(abs(heights[i] - heights[i + 1]) for i in range(len(heights) - 1))\n",
    "    return np.concatenate([flat_board, np.array(heights, dtype=np.float32), np.array([holes, bumpiness], dtype=np.float32)])\n",
    "\n",
    "class DQN_AI(nn.Module):\n",
    "    def __init__(self, state_dim=212, action_dim=10, hidden_sizes=[128, 128], lr=0.001, gamma=0.99, epsilon_start=1.0, epsilon_end=0.1):\n",
    "        super(DQN_AI, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        # Q-network\n",
    "        layers = []\n",
    "        last = state_dim\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(last, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            last = h\n",
    "        layers.append(nn.Linear(last, action_dim))\n",
    "        self.model = nn.Sequential(*layers).to(device)\n",
    "\n",
    "        # Target network\n",
    "        self.target_model = deepcopy(self.model)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.memory = []\n",
    "        self.batch_size = 128\n",
    "        self.max_memory = 10000\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, dtype=torch.float32, device=device)\n",
    "        if x.ndim == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        return self.model(x)\n",
    "\n",
    "    def act(self, state, training=True):\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.forward(state)\n",
    "            return int(torch.argmax(q_values, dim=1).item())\n",
    "\n",
    "    def get_best_move(self, board_obj, piece):\n",
    "        best_value = -float('inf')\n",
    "        best_x = 0\n",
    "        for x in range(board_obj.width):\n",
    "            try:\n",
    "                y = board_obj.drop_height(piece, x)\n",
    "                temp_board = board_obj.clone()  \n",
    "                temp_board.place(x, y, piece)\n",
    "                features = compute_board_features(temp_board)\n",
    "                value = self.forward(features).item()\n",
    "                if value > best_value:\n",
    "                    best_value = value\n",
    "                    best_x = x\n",
    "            except:\n",
    "                continue \n",
    "        return best_x, piece\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        if len(self.memory) >= self.max_memory:\n",
    "            self.memory.pop(0)\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.tensor(states, dtype=torch.float32, device=device)\n",
    "        actions = torch.tensor(actions, dtype=torch.long, device=device).unsqueeze(1)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32, device=device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "        q_vals = self.model(states).gather(1, actions)\n",
    "        with torch.no_grad():\n",
    "            q_next = self.target_model(next_states).max(1)[0].unsqueeze(1)\n",
    "            q_target = rewards + (1 - dones) * self.gamma * q_next\n",
    "\n",
    "        loss = self.loss_fn(q_vals, q_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def decay_epsilon(self, episode, max_episodes):\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon_start * (1 - episode / max_episodes))\n",
    "\n",
    "# Implemented best move function so the model plays the game properly after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17da2aee-2ec8-4c60-82cc-305d22a403c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def bool_to_np_trimmed(board):\n",
    "    trimmed_board = board[:20]  \n",
    "    return np.array(trimmed_board, dtype=np.float32).flatten()\n",
    "\n",
    "def compute_board_features(board_obj):\n",
    "    flat_board = bool_to_np_trimmed(board_obj.board)\n",
    "    heights = board_obj.heights\n",
    "    holes = 0\n",
    "    for col in range(board_obj.width):\n",
    "        block_found = False\n",
    "        for row in range(board_obj.height):\n",
    "            if board_obj.board[row][col]:\n",
    "                block_found = True\n",
    "            elif block_found:\n",
    "                holes += 1\n",
    "    bumpiness = sum(abs(heights[i] - heights[i + 1]) for i in range(len(heights) - 1))\n",
    "    max_height = max(heights)\n",
    "    return np.concatenate([flat_board, np.array(heights, dtype=np.float32),\n",
    "                           np.array([holes, bumpiness, max_height], dtype=np.float32)])\n",
    "\n",
    "class DQN_AI(nn.Module):\n",
    "    def __init__(self, state_dim=213, action_dim=10, hidden_sizes=[128, 128],\n",
    "                 lr=0.001, gamma=0.99, epsilon_start=1.0, epsilon_end=0.1):\n",
    "        super(DQN_AI, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        # Q-network\n",
    "        layers = []\n",
    "        last = state_dim\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(last, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            last = h\n",
    "        layers.append(nn.Linear(last, action_dim))\n",
    "        self.model = nn.Sequential(*layers).to(device)\n",
    "        self.target_model = deepcopy(self.model)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.memory = []\n",
    "        self.batch_size = 128\n",
    "        self.max_memory = 10000\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, dtype=torch.float32, device=device)\n",
    "        if x.ndim == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        return self.model(x)\n",
    "\n",
    "    def act(self, state, training=True):\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.forward(state)\n",
    "            return int(torch.argmax(q_values, dim=1).item())\n",
    "\n",
    "    def get_best_move(self, board_obj, piece):\n",
    "        best_value = -float('inf')\n",
    "        best_x = 0\n",
    "        for x in range(board_obj.width):\n",
    "            try:\n",
    "                y = board_obj.drop_height(piece, x)\n",
    "                temp_board = board_obj.clone()\n",
    "                temp_board.place(x, y, piece)\n",
    "                features = compute_board_features(temp_board)\n",
    "                value = self.forward(features).item()\n",
    "                if value > best_value:\n",
    "                    best_value = value\n",
    "                    best_x = x\n",
    "            except:\n",
    "                continue\n",
    "        return best_x, piece\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        if len(self.memory) >= self.max_memory:\n",
    "            self.memory.pop(0)\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.tensor(np.array(states), dtype=torch.float32, device=device)\n",
    "        actions = torch.tensor(actions, dtype=torch.long, device=device).unsqueeze(1)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "        next_states = torch.tensor(np.array(next_states), dtype=torch.float32, device=device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "        q_vals = self.model(states).gather(1, actions)\n",
    "        with torch.no_grad():\n",
    "            q_next = self.target_model(next_states).max(1)[0].unsqueeze(1)\n",
    "            q_target = rewards + (1 - dones) * self.gamma * q_next\n",
    "\n",
    "        loss = self.loss_fn(q_vals, q_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def decay_epsilon(self, episode, max_episodes):\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon_start * (1 - episode / max_episodes))\n",
    "\n",
    "    def get_reward(self, board_obj, cleared_rows):\n",
    "        holes = 0\n",
    "        for col in range(board_obj.width):\n",
    "            block_found = False\n",
    "            for row in range(board_obj.height):\n",
    "                if board_obj.board[row][col]:\n",
    "                    block_found = True\n",
    "                elif block_found:\n",
    "                    holes += 1\n",
    "        max_height = max(board_obj.heights)\n",
    "        reward = cleared_rows * 10 - holes * 2 - max_height * 0.5\n",
    "        return reward\n",
    "\n",
    "# Implemented get reward function and improved reward shaping by increasing penalties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "257e0dda-d8e1-4438-a5fa-844ecb365e00",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'piece'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcopy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deepcopy\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpiece\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Piece\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mboard\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Board\n\u001b[32m      7\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'piece'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from piece import Piece\n",
    "from board import Board\n",
    "\n",
    "class Modified_Genetic_AI:\n",
    "    def __init__(self, genotype=None, aggregate='lin', num_features=9, mutate=False, noise_sd=0.2, device=None):\n",
    "        self.device = device if device else torch.device(\"cpu\")\n",
    "        self.aggregate = aggregate\n",
    "        self.fit_score = 0.0\n",
    "\n",
    "        if genotype is None:\n",
    "            self.genotype = torch.tensor([random.uniform(-1, 1) for _ in range(num_features)],\n",
    "                                         dtype=torch.float32, device=self.device)\n",
    "        else:\n",
    "            genotype_tensor = genotype.to(self.device) if isinstance(genotype, torch.Tensor) else torch.tensor(genotype, device=self.device)\n",
    "            if mutate:\n",
    "                mutation = torch.normal(1.0, noise_sd, size=(num_features,), device=self.device)\n",
    "                self.genotype = genotype_tensor * mutation\n",
    "            else:\n",
    "                self.genotype = genotype_tensor.clone()\n",
    "\n",
    "    def valuate(self, board):\n",
    "        board_tensor = torch.tensor(board, dtype=torch.float32, device=self.device)\n",
    "        peaks = torch.max(board_tensor, dim=0)[0]\n",
    "        highest_peak = torch.max(peaks)\n",
    "        holes = torch.sum((board_tensor[:int(highest_peak), :] == 0), dim=0)\n",
    "        bumpiness = torch.sum(torch.abs(peaks[:-1] - peaks[1:]))\n",
    "        cleared = torch.sum(torch.mean(board_tensor, dim=1) > 0)\n",
    "\n",
    "        ratings = torch.tensor([\n",
    "            torch.sum(peaks),         \n",
    "            torch.sum(holes),         \n",
    "            bumpiness,                \n",
    "            torch.sum(board_tensor.sum(dim=0) == 0),\n",
    "            0, 0, 0, 0,              \n",
    "            cleared\n",
    "        ], dtype=torch.float32, device=self.device)\n",
    "\n",
    "        return torch.dot(self.genotype, ratings).item()\n",
    "\n",
    "    def get_best_move(self, board, piece):\n",
    "        best_x, max_value, best_piece = -1000, -1000, None\n",
    "        for _ in range(4):\n",
    "            piece = piece.get_next_rotation()\n",
    "            for x in range(board.width):\n",
    "                try:\n",
    "                    y = board.drop_height(piece, x)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                board_copy = deepcopy(board.board)\n",
    "                for pos in piece.body:\n",
    "                    board_copy[y + pos[1]][x + pos[0]] = True\n",
    "\n",
    "                c = self.valuate(board_copy)\n",
    "                if c > max_value:\n",
    "                    max_value = c\n",
    "                    best_x = x\n",
    "                    best_piece = piece\n",
    "        return best_x, best_piece\n",
    "\n",
    "\n",
    "\n",
    "# Testing genetic instead of reinforcement to see if I get different results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcaccda-def0-4f80-9f1b-eae2e0542ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from genetic_helpers import *\n",
    "\n",
    "class Modified_Genetic_AI:\n",
    "    def __init__(self, genotype=None, num_features=11, mutate=False, noise_sd=0.2, device='cpu'):\n",
    "        self.device = device\n",
    "        if genotype is None:\n",
    "            self.genotype = torch.rand(num_features, device=self.device) * 2 - 1\n",
    "        else:\n",
    "            if not mutate:\n",
    "                self.genotype = torch.tensor(genotype, device=self.device, dtype=torch.float32)\n",
    "            else:\n",
    "                mutation = torch.normal(1.0, noise_sd, size=(num_features,), device=self.device)\n",
    "                self.genotype = torch.tensor(genotype, device=self.device, dtype=torch.float32) * mutation\n",
    "\n",
    "        self.fit_score = 0.0\n",
    "        self.aggregate = 'lin'\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.fit_score < other.fit_score\n",
    "\n",
    "    def valuate(self, board):\n",
    "        peaks = get_peaks(board)\n",
    "        highest_peak = np.max(peaks)\n",
    "        holes = get_holes(peaks, board)\n",
    "        wells = get_wells(peaks)\n",
    "\n",
    "        row_transitions = get_row_transition(board, highest_peak)\n",
    "        col_transitions = get_col_transition(board, peaks)\n",
    "        max_height = highest_peak\n",
    "        cleared_rows = np.count_nonzero(np.mean(board, axis=1) == 1)\n",
    "\n",
    "\n",
    "        ratings = np.array([\n",
    "            -np.sum(peaks),          \n",
    "            -np.sum(holes),           \n",
    "            -get_bumpiness(peaks), \n",
    "            -np.count_nonzero(np.count_nonzero(board, axis=0) == 0), \n",
    "            -np.max(wells),           \n",
    "            -np.count_nonzero(np.array(holes) > 0), \n",
    "            -row_transitions,     \n",
    "            -col_transitions,         \n",
    "            -max_height,       \n",
    "            cleared_rows,             \n",
    "            0                 \n",
    "        ], dtype=float)\n",
    "\n",
    "        return float(np.dot(self.genotype.cpu().numpy(), ratings))\n",
    "\n",
    "    def get_best_move(self, board, piece):\n",
    "  \n",
    "        best_x = -1000\n",
    "        max_value = -1000\n",
    "        best_piece = None\n",
    "\n",
    "        for _ in range(4): \n",
    "            piece = piece.get_next_rotation()\n",
    "            for x in range(board.width):\n",
    "                try:\n",
    "                    y = board.drop_height(piece, x)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                board_copy = deepcopy(board.board)\n",
    "                for pos in piece.body:\n",
    "                    board_copy[y + pos[1]][x + pos[0]] = True\n",
    "\n",
    "                np_board = bool_to_np(board_copy)\n",
    "                val = self.valuate(np_board)\n",
    "\n",
    "                if val > max_value:\n",
    "                    max_value = val\n",
    "                    best_x = x\n",
    "                    best_piece = piece\n",
    "\n",
    "        return best_x, best_piece\n",
    "\n",
    "\n",
    "# Implemented punishments to stop the model from stacking or creating holes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73e423f-8f7c-4344-9bb2-1a2d0542d7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from genetic_helpers import *\n",
    "\n",
    "class Modified_Genetic_AI:\n",
    "    def __init__(self, genotype=None, num_features=11, mutate=False, noise_sd=0.2, device='cpu'):\n",
    "        self.device = device\n",
    "        if genotype is None:\n",
    "            self.genotype = torch.rand(num_features, device=self.device) * 2 - 1\n",
    "        else:\n",
    "            if not mutate:\n",
    "                self.genotype = torch.tensor(genotype, device=self.device, dtype=torch.float32)\n",
    "            else:\n",
    "                mutation = torch.normal(1.0, noise_sd, size=(num_features,), device=self.device)\n",
    "                self.genotype = torch.tensor(genotype, device=self.device, dtype=torch.float32) * mutation\n",
    "\n",
    "        self.fit_score = 0.0\n",
    "        self.aggregate = 'lin'\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.fit_score < other.fit_score\n",
    "\n",
    "    def valuate(self, board, total_pieces_placed=0):\n",
    "        peaks = get_peaks(board)\n",
    "        highest_peak = np.max(peaks)\n",
    "        holes = get_holes(peaks, board)\n",
    "        wells = get_wells(peaks)\n",
    "    \n",
    "        row_transitions = get_row_transition(board, highest_peak)\n",
    "        col_transitions = get_col_transition(board, peaks)\n",
    "        max_height = highest_peak\n",
    "        cleared_rows = np.count_nonzero(np.mean(board, axis=1) == 1)\n",
    "    \n",
    "        ratings = np.array([\n",
    "            -3.0 * np.sum(peaks),          \n",
    "            -4.0 * np.sum(holes),         \n",
    "            -3.0 * get_bumpiness(peaks),   \n",
    "            -1.5 * np.count_nonzero(np.count_nonzero(board, axis=0) == 0), \n",
    "            -3.0 * np.max(wells),         \n",
    "            -4.0 * np.count_nonzero(np.array(holes) > 0), \n",
    "            -2.0 * row_transitions,      \n",
    "            -2.0 * col_transitions,        \n",
    "            -3.0 * max_height,            \n",
    "            1.0 * cleared_rows,           \n",
    "            -0.05 * total_pieces_placed    \n",
    "        ], dtype=float)\n",
    "    \n",
    "        return float(np.dot(self.genotype.cpu().numpy(), ratings))\n",
    "\n",
    "\n",
    "    def get_best_move(self, board, piece):\n",
    "  \n",
    "        best_x = -1000\n",
    "        max_value = -1000\n",
    "        best_piece = None\n",
    "\n",
    "        for _ in range(4): \n",
    "            piece = piece.get_next_rotation()\n",
    "            for x in range(board.width):\n",
    "                try:\n",
    "                    y = board.drop_height(piece, x)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                board_copy = deepcopy(board.board)\n",
    "                for pos in piece.body:\n",
    "                    board_copy[y + pos[1]][x + pos[0]] = True\n",
    "\n",
    "                np_board = bool_to_np(board_copy)\n",
    "                val = self.valuate(np_board)\n",
    "\n",
    "                if val > max_value:\n",
    "                    max_value = val\n",
    "                    best_x = x\n",
    "                    best_piece = piece\n",
    "\n",
    "        return best_x, best_piece\n",
    "\n",
    "# Penalized even more and created a time penalty per piece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b236a14d-135c-4637-aab0-e581104fc24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import copy, deepcopy\n",
    "import random\n",
    "from genetic_helpers import *\n",
    "\n",
    "\n",
    "class Genetic_AI:\n",
    "    def __init__(self, genotype=None, aggregate='lin', num_features=9, mutate=False,  noise_sd=.2, move_delay=0.1):\n",
    "\n",
    "        if(genotype is None):\n",
    "            # randomly init genotype [-1, 1]\n",
    "            self.genotype = np.array([random.uniform(-1, 1) for _ in range(num_features)])\n",
    "        else:\n",
    "            if(mutate == False):\n",
    "                self.genotype = genotype\n",
    "            else:\n",
    "                # mutate given genotype\n",
    "                mutation = np.array([np.random.normal(1, noise_sd) for i in range(num_features)])\n",
    "                self.genotype = genotype * mutation\n",
    "\n",
    "        self.fit_score = 0.0\n",
    "        self.fit_rel = 0.0\n",
    "        self.aggregate = aggregate\n",
    "        \n",
    "        # Add timing controls for consistency across different PC speeds\n",
    "        self.move_delay = move_delay  # Minimum delay between moves (seconds)\n",
    "        self.last_move_time = 0\n",
    "        self.current_best_move = None\n",
    "        self.move_cache = {}  # Cache computed moves\n",
    "\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return (self.fit_score<other.fit_score)\n",
    "\n",
    "\n",
    "    def valuate(self, board, aggregate='lin'):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        peaks = get_peaks(board)\n",
    "        highest_peak = np.max(peaks)\n",
    "        holes = get_holes(peaks, board)\n",
    "        wells = get_wells(peaks)\n",
    "\n",
    "        rating_funcs = {\n",
    "            'agg_height': np.sum(peaks),\n",
    "            'n_holes': np.sum(holes),\n",
    "            'bumpiness': get_bumpiness(peaks),\n",
    "            'num_pits': np.count_nonzero(np.count_nonzero(board, axis=0) == 0),\n",
    "            'max_wells': np.max(wells),\n",
    "            'n_cols_with_holes': np.count_nonzero(np.array(holes) > 0),\n",
    "            'row_transitions': get_row_transition(board, highest_peak),\n",
    "            'col_transitions': get_col_transition(board, peaks),\n",
    "            'cleared': np.count_nonzero(np.mean(board, axis=1))\n",
    "        }\n",
    "\n",
    "        # only linear will work right now, need to extend genotype for exponents to add more\n",
    "        aggregate_funcs = {\n",
    "            'lin': lambda gene, ratings: np.dot(ratings, gene),\n",
    "            'exp': lambda gene, ratings: np.dot(np.array([ratings[i]**gene[i] for i in range(len(ratings))]), gene),\n",
    "            'disp': 0\n",
    "        }\n",
    "\n",
    "        ratings = np.array([*rating_funcs.values()], dtype=float)\n",
    "        aggregate_rating = aggregate_funcs[aggregate](self.genotype, ratings)\n",
    "\n",
    "        return aggregate_rating\n",
    "\n",
    "\n",
    "    def get_best_move(self, board, piece, force_recalculate=False):\n",
    "        \"\"\"\n",
    "        Gets the best move for an agent based on board, next piece, and genotype\n",
    "        Added caching and timing controls for consistency across different PC speeds\n",
    "        \"\"\"\n",
    "        import time\n",
    "        \n",
    "        # Check if enough time has passed since last move\n",
    "        current_time = time.time()\n",
    "        if not force_recalculate and (current_time - self.last_move_time) < self.move_delay:\n",
    "            if self.current_best_move is not None:\n",
    "                return self.current_best_move\n",
    "        \n",
    "        # Create cache key from board state and piece\n",
    "        cache_key = (tuple(map(tuple, board.board)), tuple(piece.body), tuple(piece.skirt))\n",
    "        \n",
    "        # Return cached result if available\n",
    "        if not force_recalculate and cache_key in self.move_cache:\n",
    "            return self.move_cache[cache_key]\n",
    "\n",
    "        best_x = -1000\n",
    "        max_value = -1000\n",
    "        best_piece = None\n",
    "        for i in range(4):\n",
    "            piece = piece.get_next_rotation()\n",
    "            for x in range(board.width):\n",
    "                try:\n",
    "                    y = board.drop_height(piece, x)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                board_copy = deepcopy(board.board)\n",
    "                for pos in piece.body:\n",
    "                    board_copy[y + pos[1]][x + pos[0]] = True\n",
    "\n",
    "                np_board = bool_to_np(board_copy)\n",
    "                c = self.valuate(np_board)\n",
    "\n",
    "                if c > max_value:\n",
    "                    max_value = c\n",
    "                    best_x = x\n",
    "                    best_piece = piece\n",
    "        \n",
    "        result = (best_x, best_piece)\n",
    "        \n",
    "        # Update cache and timing\n",
    "        self.move_cache[cache_key] = result\n",
    "        self.current_best_move = result\n",
    "        self.last_move_time = current_time\n",
    "        \n",
    "        # Limit cache size to prevent memory issues\n",
    "        if len(self.move_cache) > 1000:\n",
    "            # Remove oldest entries\n",
    "            self.move_cache = dict(list(self.move_cache.items())[-500:])\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "# Went back to training on base code but added a delay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac79c491-c2bd-401d-a862-1eb3b7aa2f9d",
   "metadata": {},
   "source": [
    "# 7. Present\n",
    "In a customer facing Document provide summary of finding and detail approach taken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "54a774ea-14f9-47a4-9582-443004ab4c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tasked with making a neural network that plays tetris for as long as possible  \\nThe data was outdated and using reinforcement learning, so do not need to check parameters\\nWent through 8 iterations:\\n1: Simulated gameplay with a simple neural network, but it could not learn\\n2: Improved move evaluation by adjusting parameters such as epsilon, but learning was still ineffective\\n3: Added Q-learning with replay memory so the network could remember past experiences\\n4: Implemented epsilon decay to properly update exploration and improve reward tracking\\n5: Reimplemented the best move function so the model can be tested\\n6: Ran the model overnight with 200k episodes but learning rate was too slow and could not consistently clear a row\\n7: Improved learning rate through reward shaping (penalizing holes and high stacks) and fixed a buggy simulation that caused misleading reward increases\\n8: The model accuracy and reward decreased as the training period increased\\nSwapping to a modified genetic algorithm due to having limited success from reinforcement learning\\nCurrently runs for 3 seconds and trained for 5 generations which is already better than the reinforcement learning\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Tasked with making a neural network that plays tetris for as long as possible  \n",
    "The data was outdated and using reinforcement learning, so do not need to check parameters\n",
    "Went through 11 iterations:\n",
    "1: Simulated gameplay with a simple neural network, but it could not learn\n",
    "2: Improved move evaluation by adjusting parameters such as epsilon, but learning was still ineffective\n",
    "3: Added Q-learning with replay memory so the network could remember past experiences\n",
    "4: Implemented epsilon decay to properly update exploration and improve reward tracking\n",
    "5: Reimplemented the best move function so the model can be tested\n",
    "6: Ran the model overnight with 200k episodes but learning rate was too slow and could not consistently clear a row\n",
    "7: Improved learning rate through reward shaping (penalizing holes and high stacks) and fixed a buggy simulation that caused misleading reward increases\n",
    "8: The model accuracy and reward decreased as the training period increased\n",
    "Swapping to a modified genetic algorithm due to having limited success from reinforcement learning\n",
    "Ran for a few more seconds than the reinforcement model, tested a 50 gen model and 400 gen model \n",
    "9: Implementing punishments for holes and stacking to see if it starts clearing more rows and lasting longer, didn't work\n",
    "10: Penalized even more and implemented time measures as rewards\n",
    "11: The best agents were being overwritten, so the model couldn't train properly\n",
    "12: Training on original code using 10 generations to test if the issue was in the training, also used pickle for simplicity since genetic doesn't use a neural network\n",
    "Tested original genetic code and realized that the model trained properly, but the model didn't play properly until I added a delay\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909336b7-3c7e-4eda-b3ad-aee02adfc736",
   "metadata": {},
   "source": [
    "# 8. Launch the Model System\n",
    "Define your production run code, This should be self susficent and require only your model pramaters \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2de2ebbc-4fe1-4141-8f28-37817e4f4b84",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'game'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgame\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Game\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minference\u001b[39m(n_games=\u001b[32m10\u001b[39m):\n\u001b[32m      5\u001b[39m     best_agent = torch.load(\u001b[33m\"\u001b[39m\u001b[33mbest_genetic_ai.pth\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'game'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from genetic import Genetic_AI\n",
    "from game import Game\n",
    "import pickle\n",
    "MODEL_PATH = \"10genmodel.pkl\"\n",
    "\n",
    "def inference():\n",
    "    device = \"cpu\" \n",
    "    print(\"Testing on device:\", device)\n",
    "\n",
    "    print(\"Loading saved agent...\")\n",
    "    with open(MODEL_PATH, \"rb\") as f:\n",
    "        saved_agent = pickle.load(f)\n",
    "    if isinstance(saved_agent, dict) and \"genotype\" in saved_agent:\n",
    "        genotype = saved_agent[\"genotype\"]\n",
    "    else:\n",
    "        genotype = saved_agent \n",
    "\n",
    "    agent = Genetic_AI(genotype=genotype)\n",
    "\n",
    "    game = Game(mode=\"genetic\", agent=agent)\n",
    "    score, cleared = game.run()\n",
    "\n",
    "    print(\"Final score:\", score)\n",
    "    print(\"Lines cleared:\", cleared)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    inference()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Inference code which loads the model, called test_genetic.py in actual project (tetris_a/src) and incoroprates the game, board and piece files from the tetris game\n",
    "# https://lab2.check-ai.com/user/260721/lab/tree/Machine-Learning-AI-Class/tetris_a/src/test_genetic.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747c058d-8129-4fe8-be97-10cd0c670966",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
